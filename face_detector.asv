function frameData = face_detector(video)

function extracted_img = applyFeatureExtraction(img)
    bboxes = step(lipDetector, img);
    mouthRegion = imcrop(img,bboxes(1,:));
    grayMouth = rgb2gray(mouthRegion);
    extracted_img = edge(grayMouth,'canny');
end


v=VideoReader(video);
lipDetector = vision.CascadeObjectDetector('Mouth','MergeThreshold',40);
one_frame = readFrame(v);
pointTracker = vision.PointTracker('MaxBidirectionalError', 2);
extracted_frame = applyFeatureExtraction(one_frame);
points = detectMinEigenFeatures(extracted_frame);
initialize(pointTracker, points.Location, frame);
frameData = struct('Points', {}, 'Validity', {}, 'Movement Vectors', {});
k=1;
while hasFrame(v)
    one_frame = readFrame(v);
    extracted_frame = applyFeatureExtraction(one_frame);
    [points, validity] = step(tracker, extracted_frame);
    frameData(k).Points = points;
    frameData(k).Validity = validity; 
    if frameIndex > 1
        movementVectors = points - frameData.Points{frameIndex - 1};
        allMovementVectors{frameIndex - 1} = movementVectors;
    end
    k=k+1
    
end
end


